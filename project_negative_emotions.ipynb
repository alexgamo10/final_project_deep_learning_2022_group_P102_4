{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"project_negative_emotions.ipynb","provenance":[],"collapsed_sections":["3pz4yq_jnVm5","1ExBW6RHng43","X3wmJlxO76Au","z7wXUAk-ogew","EOtKjW7qNg2l","FYpLjfA_X1md","2n6OcWZFRgUF","emXJMz5FR2By","dqtOjCD1R2Bz","m3MeiQ7aR2B2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## Analysing and Preparing Data"],"metadata":{"id":"GA6UU7EVnPjr"}},{"cell_type":"markdown","source":["###Charging Data"],"metadata":{"id":"3pz4yq_jnVm5"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jNJw80UNLzx_","outputId":"09da9126-a8ae-4bbe-cefb-405e7dae3be9","executionInfo":{"status":"ok","timestamp":1655233341211,"user_tz":-120,"elapsed":7022,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Basic example loading images from the svhn dataset\n","\n","# Dictionaries\n","import scipy.io as sio\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","import string\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","data_path = '/content/drive/MyDrive/DeepLearning_2022/PROJECT/Data/'\n","results_path = '/content/drive/MyDrive/DeepLearning_2022/PROJECT/Results/'\n","\n","import csv\n","\n","# Charge the data\n","\n","#we read the dataset Sentiment140 for tweet polarity --> 1.6M tweets\n","data_polarity = pd.read_csv(data_path+'Sentiment140.csv',encoding='utf-8')\n","\n","#we read the dataset TweetsEmotions for tweet emotions --> 9870 tweets\n","data_emotion = pd.read_csv(data_path+'TweetsEmotions.csv', index_col = 0)\n","data_emotion2 = pd.read_csv(data_path+'emotions2.csv', index_col = 0)\n","\n","#we read the datasets\n","data_emoji = pd.read_csv(data_path+'emoticons_polarity_emotion_filtered.csv',index_col = 0)"]},{"cell_type":"markdown","source":["###Preparing Data emotion data"],"metadata":{"id":"1ExBW6RHng43"}},{"cell_type":"code","source":["data_em = {\"Feeling\":[]}\n","data_em = pd.DataFrame(data_em)\n","\n","data_em[\"Feeling\"]=data_emotion2[\"Feeling\"]\n","data_em.index.name = 'Tweets'\n","data_em.reset_index(inplace=True)"],"metadata":{"id":"YtCz6hxPceS6","executionInfo":{"status":"ok","timestamp":1655233341211,"user_tz":-120,"elapsed":34,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(data_em)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVO1GV5uprVg","outputId":"d5b24816-9b15-4573-a85d-398b4d823d01","executionInfo":{"status":"ok","timestamp":1655233341212,"user_tz":-120,"elapsed":32,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  Tweets  Feeling\n","0                                i didnt feel humiliated  sadness\n","1      i can go from feeling so hopeless to so damned...  sadness\n","2       im grabbing a minute to post i feel greedy wrong    anger\n","3      i am ever feeling nostalgic about the fireplac...     love\n","4                                   i am feeling grouchy    anger\n","...                                                  ...      ...\n","15995  i just had a very brief time in the beanbag an...  sadness\n","15996  i am now turning and i feel pathetic that i am...  sadness\n","15997                     i feel strong and good overall      joy\n","15998  i feel like this was such a rude comment and i...    anger\n","15999  i know a lot but i feel so stupid because i ca...  sadness\n","\n","[16000 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["print(data_em.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0D7I1mR0QjPl","outputId":"28cc07b6-a2cc-45a0-bcb5-b88f4311262f","executionInfo":{"status":"ok","timestamp":1655233341213,"user_tz":-120,"elapsed":30,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['Tweets', 'Feeling'], dtype='object')\n"]}]},{"cell_type":"code","source":["\n","# Again not all types have the same amount of data, dangerous\n","print(data_em[\"Feeling\"].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z78Nd9rwQwbI","outputId":"0a884deb-6756-453d-9d34-28b650741d02","executionInfo":{"status":"ok","timestamp":1655233341213,"user_tz":-120,"elapsed":28,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["joy         5362\n","sadness     4666\n","anger       2159\n","fear        1937\n","love        1304\n","surprise     572\n","Name: Feeling, dtype: int64\n"]}]},{"cell_type":"code","source":["for i in range(len(data_em)):\n","  if data_em.loc[i,'Feeling']=='sadness':\n","    data_em.loc[i,'Feeling'] = 'sad'\n","  elif data_em.loc[i,'Feeling']=='anger':\n","    data_em.loc[i,'Feeling'] = 'angry'\n"],"metadata":{"id":"5E4I2K4ck6R-","executionInfo":{"status":"ok","timestamp":1655233347576,"user_tz":-120,"elapsed":6389,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(data_em[data_em['Feeling']=='sad'])\n","print(data_em[data_em['Feeling']=='angry'])\n","print(data_em[data_em['Feeling']=='fear'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NiOFStPhtFAf","outputId":"94b2f469-b1c4-4041-819d-885a0eccae55","executionInfo":{"status":"ok","timestamp":1655233347577,"user_tz":-120,"elapsed":33,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  Tweets Feeling\n","0                                i didnt feel humiliated     sad\n","1      i can go from feeling so hopeless to so damned...     sad\n","5      ive been feeling a little burdened lately wasn...     sad\n","10     i feel like i have to make the suffering i m s...     sad\n","13                    i feel low energy i m just thirsty     sad\n","...                                                  ...     ...\n","15988            i feel pathetic because im still single     sad\n","15991  i have wanted to perhaps convey my feelings of...     sad\n","15995  i just had a very brief time in the beanbag an...     sad\n","15996  i am now turning and i feel pathetic that i am...     sad\n","15999  i know a lot but i feel so stupid because i ca...     sad\n","\n","[4666 rows x 2 columns]\n","                                                  Tweets Feeling\n","2       im grabbing a minute to post i feel greedy wrong   angry\n","4                                   i am feeling grouchy   angry\n","12     i think it s the easiest time of year to feel ...   angry\n","20     i feel irritated and rejected without anyone d...   angry\n","24     i already feel like i fucked up though because...   angry\n","...                                                  ...     ...\n","15967          i have a feeling im going to be heartless   angry\n","15977  i often find my self feeling offended myself w...   angry\n","15982  i took a minute to appreciate the trees around...   angry\n","15992  i moved away he said something that made me fe...   angry\n","15998  i feel like this was such a rude comment and i...   angry\n","\n","[2159 rows x 2 columns]\n","                                                  Tweets Feeling\n","7      i feel as confused about life as a teenager or...    fear\n","19     i now feel compromised and skeptical of the va...    fear\n","21     i am feeling completely overwhelmed i have two...    fear\n","31     i remember feeling acutely distressed for a fe...    fear\n","53     i was stymied a little bit as i wrote feeling ...    fear\n","...                                                  ...     ...\n","15962  i can understand feeling uncertain about the a...    fear\n","15975  i inspect samples of wheat i started feeling t...    fear\n","15978  i remember waking up feeling anxious and excit...    fear\n","15979  i have writer s block or feel too apprehensive...    fear\n","15983  im still feeling all wimpy it may be another s...    fear\n","\n","[1937 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["data_e = data_emotion[['Tweets','Feeling']]\n","print(data_e)\n","print(data_e['Feeling'].value_counts())\n","data_e = data_e.append(data_em[data_em['Feeling']=='sad'].sample(n=700),ignore_index=True)\n","data_e = data_e.append(data_em[data_em['Feeling']=='angry'],ignore_index=True)\n","data_e = data_e.append(data_em[data_em['Feeling']=='fear'],ignore_index=True)\n","print(data_e)\n","print(data_e['Feeling'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c0c2775-2d28-4e91-fda8-6738d943ac5c","id":"WtCbUki1kMoH","executionInfo":{"status":"ok","timestamp":1655233347577,"user_tz":-120,"elapsed":29,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  Tweets Feeling\n","Sl no                                                           \n","1       #1: @fe ed \"RT @MirayaDizon1: Time is ticking...   happy\n","2       #2: @蓮花 &はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...   happy\n","3       #3: @Ris ♡ ed \"Happy birthday to one smokin h...   happy\n","4       #4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...   happy\n","5       #5: @Madhurima wth u vc♥ ed \"Good morning dea...   happy\n","...                                                  ...     ...\n","10016  Tweet #85: @Matteo tweeted \"@GameSpot @Frannkc...   angry\n","10017  Tweet #86: @𝐚𝐧𝐢𝐬𝐭𝐨𝐧 tweeted \"@BRATgiirl_ that’...   angry\n","10018  Tweet #87: @Chowkidar Ricky Sharma tweeted \"@M...   angry\n","10019  Tweet #88: @Katoe.EXE tweeted \"u know what i h...   angry\n","10019  Tweet #88: @Katoe.EXE tweeted \"u know what i h...   angry\n","\n","[10017 rows x 2 columns]\n","happy       3928\n","sad         2849\n","angry       1341\n","fear         863\n","disgust      637\n","surprise     399\n","Name: Feeling, dtype: int64\n","                                                  Tweets Feeling\n","0       #1: @fe ed \"RT @MirayaDizon1: Time is ticking...   happy\n","1       #2: @蓮花 &はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...   happy\n","2       #3: @Ris ♡ ed \"Happy birthday to one smokin h...   happy\n","3       #4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...   happy\n","4       #5: @Madhurima wth u vc♥ ed \"Good morning dea...   happy\n","...                                                  ...     ...\n","14808  i can understand feeling uncertain about the a...    fear\n","14809  i inspect samples of wheat i started feeling t...    fear\n","14810  i remember waking up feeling anxious and excit...    fear\n","14811  i have writer s block or feel too apprehensive...    fear\n","14812  im still feeling all wimpy it may be another s...    fear\n","\n","[14813 rows x 2 columns]\n","happy       3928\n","sad         3549\n","angry       3500\n","fear        2800\n","disgust      637\n","surprise     399\n","Name: Feeling, dtype: int64\n"]}]},{"cell_type":"code","source":["data_e = data_e.drop(10019)"],"metadata":{"id":"b_kuhDMUkMoK","executionInfo":{"status":"ok","timestamp":1655233347577,"user_tz":-120,"elapsed":9,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["data_e['Feeling'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f441332e-3326-4bd0-a584-911c8511f993","id":"6KaD3btrkMoM","executionInfo":{"status":"ok","timestamp":1655233347578,"user_tz":-120,"elapsed":9,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["happy       3928\n","sad         3548\n","angry       3500\n","fear        2800\n","disgust      637\n","surprise     399\n","Name: Feeling, dtype: int64"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from sklearn.utils import shuffle\n","data_e= shuffle(data_e)"],"metadata":{"id":"zOORPhFKkMoO","executionInfo":{"status":"ok","timestamp":1655233347578,"user_tz":-120,"elapsed":8,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["data_e['Feeling'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZC56kRfr2fc","outputId":"7293bd16-27e7-46c7-ef2d-b33233f5a0a8","executionInfo":{"status":"ok","timestamp":1655233347579,"user_tz":-120,"elapsed":9,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["happy       3928\n","sad         3548\n","angry       3500\n","fear        2800\n","disgust      637\n","surprise     399\n","Name: Feeling, dtype: int64"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["for i, row in data_e.iterrows():\n","  if data_e.loc[i, \"Feeling\"] == \"happy\" or data_e.loc[i, \"Feeling\"] == \"surprise\":\n","    data_e.loc[i, \"Polarity\"] = 1\n","  elif data_e.loc[i, \"Feeling\"] == \"sad\" or data_e.loc[i, \"Feeling\"] == \"angry\" or data_e.loc[i, \"Feeling\"] == \"fear\" or data_e.loc[i, \"Feeling\"] == \"disgust\":\n","    data_e.loc[i, \"Polarity\"] = 0\n","\n","data_e.reindex()\n","data_e_0 = data_e[data_e[\"Polarity\"] == 0].reindex()\n","data_e_1 = data_e[data_e[\"Polarity\"] == 1].reindex()"],"metadata":{"id":"ax2LRvx7tUEz","executionInfo":{"status":"ok","timestamp":1655233359526,"user_tz":-120,"elapsed":11955,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["data_e[\"Polarity\"].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtXIGQj-iHos","outputId":"021b744f-78d2-4f7c-f42b-b69198ddf345","executionInfo":{"status":"ok","timestamp":1655233359527,"user_tz":-120,"elapsed":30,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0    10485\n","1.0     4327\n","Name: Polarity, dtype: int64"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["for i,row  in data_e_0.iterrows():\n","  if data_e_0.loc[i, \"Feeling\"] == \"sad\":\n","    data_e_0.loc[i, \"Feeling\"] = 0\n","  elif data_e_0.loc[i, \"Feeling\"] == \"angry\":\n","    data_e_0.loc[i, \"Feeling\"] = 1\n","  elif data_e_0.loc[i, \"Feeling\"] == \"fear\":\n","    data_e_0.loc[i, \"Feeling\"] = 2\n","  elif data_e_0.loc[i, \"Feeling\"] == \"disgust\":\n","    data_e_0.loc[i, \"Feeling\"] = 3\n","\n","for i,row  in data_e_1.iterrows():\n","  if data_e_1.loc[i, \"Feeling\"] == \"happy\":\n","    data_e_1.loc[i, \"Feeling\"] = 1\n","  elif data_e_1.loc[i, \"Feeling\"] == \"surprise\":\n","    data_e_1.loc[i, \"Feeling\"] = 0"],"metadata":{"id":"kS2f7byNub52","executionInfo":{"status":"ok","timestamp":1655233368095,"user_tz":-120,"elapsed":8594,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["data_e_0[\"Feeling\"].value_counts()\n","\n","print(data_e_0[\"Feeling\"].value_counts())\n","data_e_0_s = data_e_0[data_e_0[\"Feeling\"] == 0]\n","data_e_0_a = data_e_0[data_e_0[\"Feeling\"] == 1]\n","data_e_0_f = data_e_0[data_e_0[\"Feeling\"] == 2]\n","data_e_0_d = data_e_0[data_e_0[\"Feeling\"] == 3]\n","\n","data_e_0_s = data_e_0_s.sample(n=650)\n","data_e_0_a = data_e_0_a.sample(n=650)\n","data_e_0_f = data_e_0_f.sample(n=650)\n","data_e_0_d = data_e_0_d.sample(n=637)\n","\n","data_e_0 = data_e_0_s.append(data_e_0_a,ignore_index=True)\n","data_e_0 = data_e_0.append(data_e_0_f,ignore_index=True)\n","data_e_0 = data_e_0.append(data_e_0_d,ignore_index=True)\n","\n","data_e_0 = shuffle(data_e_0)\n","\n","print(data_e_0[\"Feeling\"].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJhqzggwc8z2","outputId":"ca494902-793c-4662-db87-1fb599c6f7b6","executionInfo":{"status":"ok","timestamp":1655233368095,"user_tz":-120,"elapsed":19,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["0    3548\n","1    3500\n","2    2800\n","3     637\n","Name: Feeling, dtype: int64\n","1    650\n","2    650\n","0    650\n","3    637\n","Name: Feeling, dtype: int64\n"]}]},{"cell_type":"markdown","source":["###Preparing Data Emotion Polarity 0"],"metadata":{"id":"X3wmJlxO76Au"}},{"cell_type":"code","source":["import re\n","from string import punctuation\n","\n","all_tweets_e_0 = list()\n","\n","for t in data_e_0[\"Tweets\"]:\n","  #t = 'a ' + t\n","  t = re.sub(\"(?:\\s)@[^, ]*\", '', t)\n","  #t = re.sub(\"(?:\\s)#[^, ]*\", '', t)\n","  t = t[4:]\n","  t = t.lower()\n","  t = \"\".join([ch for ch in t if ch not in punctuation])\n","  all_tweets_e_0.append(t)\n","\n","all_text = \" \".join(all_tweets_e_0)\n","all_words = all_text.split()"],"metadata":{"id":"d66zPUGV7_PT","executionInfo":{"status":"ok","timestamp":1655233368095,"user_tz":-120,"elapsed":18,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from collections import Counter \n","# Count all the words using Counter Method\n","count_words = Counter(all_words)\n","total_words=len(all_words)\n","sorted_words=count_words.most_common(total_words)\n","print(\"Top ten occuring words : \",sorted_words[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkryiTIL8FRf","outputId":"a57ff92a-cb1f-4cbe-bc8a-608b6e9ef2e9","executionInfo":{"status":"ok","timestamp":1655233368096,"user_tz":-120,"elapsed":18,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Top ten occuring words :  [('the', 1445), ('ed', 1339), ('to', 1305), ('a', 1187), ('and', 1164), ('i', 1110), ('rt', 840), ('of', 800), ('in', 552), ('is', 551)]\n"]}]},{"cell_type":"code","source":["vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}"],"metadata":{"id":"DeWY5ofr8H3E","executionInfo":{"status":"ok","timestamp":1655233368096,"user_tz":-120,"elapsed":18,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["encoded_tweets=list()\n","iter_1 = 0\n","iter_2 = 0\n","for t in all_tweets_e_0:\n","  encoded_tweet=list()\n","  for word in t.split():\n","    if word not in vocab_to_int.keys():\n","      #if word is not available in vocab_to_int put 0 in that place\n","      encoded_tweet.append(0)\n","      iter_1 += 1\n","    else:\n","      iter_2 += 1\n","      encoded_tweet.append(vocab_to_int[word])\n","  encoded_tweets.append(encoded_tweet)\n","print(iter_1)\n","print(iter_2)\n","print(encoded_tweets[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLk4IQl88Kkj","outputId":"29c2a498-f023-4229-ba5d-d23ec1cb405e","executionInfo":{"status":"ok","timestamp":1655233368096,"user_tz":-120,"elapsed":17,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","55832\n","[79, 24, 571, 5, 1688]\n"]}]},{"cell_type":"code","source":["sequence_length = 0\n","for i, tweet in enumerate(encoded_tweets):\n","  if len(tweet) > sequence_length:\n","    sequence_length = len(tweet)\n","\n","features_e_0 = []\n","for i, tweet in enumerate(encoded_tweets):\n","  tweet_len=len(tweet)\n","  if (tweet_len<=sequence_length):\n","    zeros=list(np.zeros(sequence_length-tweet_len))\n","    new=zeros+tweet\n","  else:\n","    new=tweet[:sequence_length]\n","  features_e_0.append(np.array(new))"],"metadata":{"id":"6Ud9hhUg8NFS","executionInfo":{"status":"ok","timestamp":1655233368097,"user_tz":-120,"elapsed":18,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["### Preparing Data Emoji"],"metadata":{"id":"zdG5dVwbn4rs"}},{"cell_type":"code","source":["# Again not all types have the same number of data, dangerous\n","print(data_emoji)\n","print(data_emoji[\"emotion\"].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXV2TH6RtLIH","outputId":"95542c8e-b3c5-427c-e582-6c362260eb97","executionInfo":{"status":"ok","timestamp":1655233368097,"user_tz":-120,"elapsed":17,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  tweet  \\\n","0     The woman who was forced into child labour dur...   \n","1     A group of children who fled #Mariupol are now...   \n","2     @MZavala86 @RT_com Dictator Vladimir Putin has...   \n","3     Yesterday in Brussels during a peaceful strike...   \n","4     😡 The Russians who left the #Chornobyl nuclear...   \n","...                                                 ...   \n","2068  Let’s stop the russian spelling and name it th...   \n","2069  #Lysychansk after #Putin 🤬 #Лисичанськ #Україн...   \n","2070  This is really brutal! 🤬\\n\\nA Ukrainian office...   \n","2071  Hey #Germany! Are you going to start using #pe...   \n","2072  Go figure ...India won't condemn Russia for (t...   \n","\n","                          emoji  polarity  emotion  \n","0                         ['😭']         0        3  \n","1                         ['😭']         0        3  \n","2                         ['😂']         1        1  \n","3                         ['😓']         0        3  \n","4                         ['😡']         0        4  \n","...                         ...       ...      ...  \n","2068                      ['😍']         1        1  \n","2069                      ['🤬']         0        4  \n","2070                      ['🤬']         0        4  \n","2071  ['🤣', '😂', '🤣', '😂', '🤣']         1        1  \n","2072                      ['😬']         0        6  \n","\n","[2073 rows x 4 columns]\n","1    1103\n","3     496\n","4     400\n","6      46\n","2      23\n","5       5\n","Name: emotion, dtype: int64\n"]}]},{"cell_type":"code","source":["data_emoji_0 = data_emoji[data_emoji[\"polarity\"]==0]\n","data_emoji_0[\"emotion\"].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bv6ufCUQRBIL","outputId":"454790a2-4e06-4cdf-b35f-ec2473d00eb1","executionInfo":{"status":"ok","timestamp":1655233368097,"user_tz":-120,"elapsed":16,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3    496\n","4    400\n","6     46\n","5      5\n","Name: emotion, dtype: int64"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["for i,row in data_emoji_0.iterrows():\n","  if (data_emoji_0.loc[i,\"emotion\"]==3):\n","    data_emoji_0.loc[i,\"emotion\"]=0\n","  elif (data_emoji_0.loc[i,\"emotion\"]==4):\n","    data_emoji_0.loc[i,\"emotion\"]=1\n","  elif (data_emoji_0.loc[i,\"emotion\"]==5):\n","    data_emoji_0.loc[i,\"emotion\"]=2\n","  elif (data_emoji_0.loc[i,\"emotion\"]==6):\n","    data_emoji_0.loc[i,\"emotion\"]=3 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GybFVtNlRpDI","outputId":"11f4773b-9ef7-459c-d349-135dc8aa0690","executionInfo":{"status":"ok","timestamp":1655233368450,"user_tz":-120,"elapsed":367,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_single_column(loc, value, pi)\n"]}]},{"cell_type":"code","source":["data_emoji_0[\"emotion\"].value_counts()\n","\n","print(data_emoji_0[\"emotion\"].value_counts())\n","data_emoji_0_s = data_emoji_0[data_emoji_0[\"emotion\"] == 0]\n","data_emoji_0_a = data_emoji_0[data_emoji_0[\"emotion\"] == 1]\n","data_emoji_0_f = data_emoji_0[data_emoji_0[\"emotion\"] == 2]\n","data_emoji_0_d = data_emoji_0[data_emoji_0[\"emotion\"] == 3]\n","\n","data_emoji_0_s = data_emoji_0_s.sample(n=46)\n","data_emoji_0_a = data_emoji_0_a.sample(n=46)\n","data_emoji_0_f = data_emoji_0_f.sample(n=5)\n","data_emoji_0_d = data_emoji_0_d.sample(n=46)\n","\n","data_emoji_0 = data_emoji_0_s.append(data_emoji_0_a,ignore_index=True)\n","data_emoji_0 = data_emoji_0.append(data_emoji_0_f,ignore_index=True)\n","data_emoji_0 = data_emoji_0.append(data_emoji_0_d,ignore_index=True)\n","\n","data_emoji_0 = shuffle(data_emoji_0)\n","\n","print(data_emoji_0[\"emotion\"].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYCK7TkPR94U","outputId":"eefd55da-7462-41b9-a73a-b4c45ee4684b","executionInfo":{"status":"ok","timestamp":1655233368450,"user_tz":-120,"elapsed":6,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["0    496\n","1    400\n","3     46\n","2      5\n","Name: emotion, dtype: int64\n","3    46\n","1    46\n","0    46\n","2     5\n","Name: emotion, dtype: int64\n"]}]},{"cell_type":"markdown","source":["###Analysing tweets"],"metadata":{"id":"z7wXUAk-ogew"}},{"cell_type":"code","source":["#split_dataset into 80% training , 10% test and 10% Validation Dataset\n","'''train_x=features[:int(0.8*len(features))]\n","train_y=data_p_r[\"polarity\"][:int(0.8*len(features))]\n","valid_x=features[int(0.8*len(features)):int(0.9*len(features))]\n","valid_y=data_p_r[\"polarity\"][int(0.8*len(features)):int(0.9*len(features))]\n","test_x=features[int(0.9*len(features)):]\n","test_y=data_p_r[\"polarity\"][int(0.9*len(features)):]'''\n","\n","train_x=features_e_0[:int(0.7*len(features_e_0))]\n","train_y=data_e_0[\"Feeling\"][:int(0.7*len(features_e_0))]\n","valid_x=features_e_0[int(0.7*len(features_e_0)):int(0.8*len(features_e_0))]\n","valid_y=data_e_0[\"Feeling\"][int(0.7*len(features_e_0)):int(0.8*len(features_e_0))]\n","test_x=features_e_0[int(0.8*len(features_e_0)):]\n","test_y=data_e_0[\"Feeling\"][int(0.8*len(features_e_0)):]\n","print(len(train_y), len(valid_y), len(test_y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbTgrc_bhrrI","outputId":"d3b2155b-d8ed-4b2a-959f-94a82c932c46","executionInfo":{"status":"ok","timestamp":1655233368451,"user_tz":-120,"elapsed":5,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["1810 259 518\n"]}]},{"cell_type":"markdown","source":["##Train the model"],"metadata":{"id":"KGD7_rFAo6OZ"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","import torch\n","import torch.nn.functional as F\n","\n","#create Tensor Dataset\n","train_data=TensorDataset(torch.FloatTensor(train_x).to(torch.int64), F.one_hot(torch.FloatTensor(train_y.to_numpy(dtype=np.float64)).to(torch.int64)))\n","valid_data=TensorDataset(torch.FloatTensor(valid_x).to(torch.int64), F.one_hot(torch.FloatTensor(valid_y.to_numpy(dtype=np.float64)).to(torch.int64)))\n","test_data=TensorDataset(torch.FloatTensor(test_x).to(torch.int64), F.one_hot(torch.FloatTensor(test_y.to_numpy(dtype=np.float64)).to(torch.int64)))\n","\n","#dataloader\n","batch_size=64\n","train_loader=DataLoader(train_data, batch_size=1, shuffle=True)\n","valid_loader=DataLoader(valid_data, batch_size=1, shuffle=True)\n","test_loader=DataLoader(test_data, batch_size=1, shuffle=True)"],"metadata":{"id":"SgqnpDiuiz_W","executionInfo":{"status":"ok","timestamp":1655233368964,"user_tz":-120,"elapsed":516,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"07cb692e-634d-4544-87d9-52532a72b013"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n","  \n"]}]},{"cell_type":"code","source":["# obtain one batch of training data\n","dataiter = iter(train_loader)\n","sample_x, sample_y = dataiter.next()\n","print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n","print('Sample input: \\n', sample_x)\n","print('Sample label size: ', sample_y.size()) # batch_size\n","print('Sample label: \\n', sample_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xwr04MILkJ1E","outputId":"dfd81b8b-d5f7-4c3c-a576-488bcdde034b","executionInfo":{"status":"ok","timestamp":1655233368965,"user_tz":-120,"elapsed":9,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample input size:  torch.Size([1, 66])\n","Sample input: \n"," tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,   23,   25,    6,\n","           35, 1423,   54,    4, 7134, 7135]])\n","Sample label size:  torch.Size([1, 4])\n","Sample label: \n"," tensor([[0, 0, 1, 0]])\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n"," \n","class SentimentalLSTM(nn.Module):\n","    \"\"\"\n","    The RNN model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n","        \"\"\"\n","        Initialize the model by setting up the layers\n","        \"\"\"\n","        super().__init__()\n","        self.output_size=output_size\n","        self.n_layers=n_layers\n","        self.hidden_dim=hidden_dim\n","        \n","        #Embedding and LSTM layers\n","        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","        \n","        #dropout layer\n","        self.dropout=nn.Dropout(0.4)\n","        \n","        #Linear and sigmoid layer\n","        self.fc1=nn.Linear(hidden_dim, 64)\n","        self.fc2=nn.Linear(64, 16)\n","        self.fc3=nn.Linear(16,output_size)\n","        self.sigmoid=nn.Sigmoid()\n","        self.softAct = nn.Softmax()\n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size=x.size()\n","        \n","        #Embadding and LSTM output\n","        embedd=self.embedding(x)\n","        lstm_out, hidden=self.lstm(embedd, hidden)\n","\n","        #stack up the lstm output\n","        # lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n","\n","        #dropout and fully connected layers\n","        out=self.dropout(lstm_out)\n","        out=self.fc1(out)\n","        out=self.dropout(out)\n","        out=self.fc2(out)\n","        out=self.dropout(out)\n","        out=self.fc3(out)\n","       \n","        \n","       # out = self.fc3(out)\n","        sig_out = self.sigmoid(out)\n","        sig_out = sig_out[:,-1,:]\n","        \n","\n","        '''\n","        print(out.contiguous().view(-1).shape)\n","       # sig_out = self.softAct(out[:,-1]).view(batch_size, -1)\n","        sig_out=out.contiguous().view(-1)\n","        print(sig_out.shape)\n","        sig_out=sig_out.view(batch_size,-1)\n","        print(sig_out.shape)'''\n","        return sig_out, hidden\n","    \n","    def init_hidden(self, batch_size):\n","        \"\"\"Initialize Hidden STATE\"\"\"\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        return (torch.zeros(1, 1, hidden_dim).cuda(),\n","                torch.zeros(1, 1, hidden_dim).cuda())\n","        "],"metadata":{"id":"i-kB0lgqi07c","executionInfo":{"status":"ok","timestamp":1655233369308,"user_tz":-120,"elapsed":348,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Instantiate the model w/ hyperparams\n","vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n","output_size = 4\n","embedding_dim = 400\n","hidden_dim = 256\n","n_layers = 1\n","\n","net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","print(net)"],"metadata":{"id":"sKSQp-Qoormu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a19bac5d-9a52-4124-f905-f13da041556b","executionInfo":{"status":"ok","timestamp":1655233369309,"user_tz":-120,"elapsed":12,"user":{"displayName":"alex garcia montane","userId":"08010763190334865161"}}},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["SentimentalLSTM(\n","  (embedding): Embedding(10517, 400)\n","  (lstm): LSTM(400, 256, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.4, inplace=False)\n","  (fc1): Linear(in_features=256, out_features=64, bias=True)\n","  (fc2): Linear(in_features=64, out_features=16, bias=True)\n","  (fc3): Linear(in_features=16, out_features=4, bias=True)\n","  (sigmoid): Sigmoid()\n","  (softAct): Softmax(dim=None)\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","source":["# loss and optimization functions\n","lr=0.0005\n","\n","#criterion = nn.CrossEntropyLoss(reduction='sum')\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0001)\n","\n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","# training params\n","\n","epochs = 20# 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","training_loss=[]\n","validation_loss=[]\n","\n","counter = 0\n","print_every = 200\n","clip=5 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","for e in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    # batch loop\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            inputs=inputs.cuda()\n","            labels=labels.cuda()\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        net.zero_grad()\n","        # get the output from the model\n","        output, h = net(inputs, h)\n","        # calculate the loss and perform backprop\n","        loss = criterion(output, labels.float())\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","        # loss stats\n","        if counter % print_every == 0:\n","            # Get validation loss\n","            val_h = net.init_hidden(batch_size)\n","            val_losses = []\n","            net.eval()\n","            error = 0\n","            for inputs, labels in valid_loader:\n","\n","                # Creating new variables for the hidden state, otherwise\n","                # we'd backprop through the entire training history\n","                val_h = tuple([each.data for each in val_h])\n","                if(train_on_gpu):\n","                  inputs, labels = inputs.cuda(), labels.cuda()\n","\n","                output, val_h = net(inputs, val_h)\n","                val_loss = criterion(output, labels.float())\n","                val_losses.append(val_loss.item())\n","                 \n","            training_loss.append(loss)\n","            validation_loss.append(np.mean(val_losses))\n","            net.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Loss: {:.6f}...\".format(loss.item()),\n","                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzK5k52uove6","outputId":"c1229c69-162e-4358-8fec-da7950392e20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/20... Step: 200... Loss: 1.428745... Val Loss: 1.377759\n","Epoch: 1/20... Step: 400... Loss: 1.261500... Val Loss: 1.326077\n"]}]},{"cell_type":"code","source":["t1 = []\n","for i in training_loss:\n","  t1.append(i.cpu().item())\n","\n","\n","plt.plot(t1,'yellow', label=\"training loss\")\n","plt.plot(validation_loss, 'purple', label=\"validation loss\")\n","plt.legend()\n","plt.show()\n","\n","t = []\n","v = []\n","for i in range(0,20):\n","  v.append(np.mean(validation_loss[i*7:(i+1)*7]))\n","  t.append(np.mean(t1[i*7:(i+1)*7]))\n","\n","plt.plot(t,'red',label='training loss')\n","plt.plot(v, 'blue', label='validation loss')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"qY_mV5v7q9Dm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from functools import total_ordering\n","test_losses = [] # track loss\n","num_correct = 0\n","num_sad_correct = 0\n","num_anger_correct = 0\n","num_fear_correct = 0\n","num_disgust_correct = 0\n","total_s = 0\n","total_a = 0\n","total_f = 0\n","total_d = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.eval()\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","  # Creating new variables for the hidden state, otherwise\n","  # we'd backprop through the entire training history\n","  h = tuple([each.data for each in h])\n","  e=0\n","\n","  if(train_on_gpu):\n","    inputs, labels_2 = inputs.cuda(), labels.cuda()\n","\n","  output, h = net(inputs, h)\n","\n","  # calculate loss\n","  test_loss = criterion(output, labels_2.float())\n","  test_losses.append(test_loss.item())\n","\n","  highest_value = 0\n","  high = 0\n","  counter = 0\n","  for i in output[0].cpu().detach().numpy():\n","    if i > highest_value:\n","      highest_value = i\n","      high = counter\n","    counter += 1\n","  \n","  pred = [0,0,0,0]\n","  pred[high] = 1\n","\n","\n","  # convert output probabilities to predicted class (0 or 1)\n","  #pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","  # compare predictions to true label\n","  #correct_tensor = pred.eq(labels.view_as(pred))\n","\n","  if labels[0].numpy()[0]==1:\n","    total_s +=1\n","    if pred[0] == 1:\n","      num_sad_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[1]==1:\n","    total_a +=1\n","    if pred[1] == 1:\n","      num_anger_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[2]==1:\n","    total_f +=1\n","    if pred[2] == 1:\n","      num_fear_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[3]==1:\n","    total_d +=1\n","    if pred[3] == 1:\n","      num_disgust_correct += 1\n","      num_correct += 1\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","\n","test_acc = num_correct/len(test_loader.dataset)\n","test_acc_s = num_sad_correct/total_s\n","test_acc_f = num_fear_correct/total_f\n","test_acc_a = num_anger_correct/total_a\n","test_acc_d = num_disgust_correct/total_d\n","print(\"Test accuracy: {:.3f}\".format(test_acc))\n","print(\"Test sad accuracy: {:.6f}\".format(test_acc_s))\n","print(\"Test anger accuracy: {:.6f}\".format(test_acc_a))\n","print(\"Test fear accuracy: {:.6f}\".format(test_acc_f))\n","print(\"Test disgust accuracy: {:.6f}\".format(test_acc_d))"],"metadata":{"id":"gCxwjlDjpXqE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Clean the Ucraine-Russia dataset and use it for negative emotions testing"],"metadata":{"id":"EOtKjW7qNg2l"}},{"cell_type":"code","source":["import re\n","\n","all_tweets_u = list()\n","for t in data_emoji_0[\"tweet\"]:\n","  t = 'a ' + t\n","  t = re.sub(\"(?:\\s)@[^, ]*\", '', t)\n","  t = t[2:]\n","  t = t.lower()\n","  t = \"\".join([ch for ch in t if ch not in punctuation])\n","  all_tweets_u.append(t)\n","all_text = \" \".join(all_tweets_u)\n","all_words = all_text.split()"],"metadata":{"id":"TkbQsk4fNg2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_tweets=list()\n","iter_1 = 0\n","iter_2 = 0\n","for t in all_tweets_u:\n","  encoded_tweet=list()\n","  for word in t.split():\n","    if word not in vocab_to_int.keys():\n","      #if word is not available in vocab_to_int put 0 in that place\n","      encoded_tweet.append(0)\n","      iter_1 += 1\n","    else:\n","      iter_2 += 1\n","      encoded_tweet.append(vocab_to_int[word])\n","  encoded_tweets.append(encoded_tweet)\n","print(iter_1)\n","print(iter_2)\n","print(encoded_tweets[0])"],"metadata":{"id":"Rc65lkQzNg2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequence_length = 0\n","for i, tweet in enumerate(encoded_tweets):\n","  if len(tweet) > sequence_length:\n","    sequence_length = len(tweet)\n","\n","features = []\n","for i, tweet in enumerate(encoded_tweets):\n","  tweet_len=len(tweet)\n","  if (tweet_len<=sequence_length):\n","    zeros=list(np.zeros(sequence_length-tweet_len))\n","    new=zeros+tweet\n","  else:\n","    new=tweet[:sequence_length]\n","  features.append(np.array(new))"],"metadata":{"id":"YWGzsT97Ng2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","test_x_u=features[:int(len(features))]\n","test_y_u=data_emoji_0[\"emotion\"][:int(len(features))]\n","\n","#create Tensor Dataset\n","test_data_u=TensorDataset(torch.FloatTensor(test_x_u).to(torch.int64), F.one_hot(torch.FloatTensor(test_y_u.to_numpy(dtype=np.float64)).to(torch.int64)))\n","\n","#dataloader\n","batch_size=64\n","test_loader_u=DataLoader(test_data_u, batch_size=1, shuffle=True)"],"metadata":{"id":"m97zOe3jNg2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from functools import total_ordering\n","test_losses = [] # track loss\n","num_correct = 0\n","num_sad_correct = 0\n","num_anger_correct = 0\n","num_fear_correct = 0\n","num_disgust_correct = 0\n","total_s = 0\n","total_a = 0\n","total_f = 0\n","total_d = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.eval()\n","# iterate over test data\n","for inputs, labels in test_loader_u:\n","\n","  # Creating new variables for the hidden state, otherwise\n","  # we'd backprop through the entire training history\n","  h = tuple([each.data for each in h])\n","  e=0\n","\n","  if(train_on_gpu):\n","    inputs, labels_2 = inputs.cuda(), labels.cuda()\n","\n","  output, h = net(inputs, h)\n","\n","  # calculate loss\n","  test_loss = criterion(output, labels_2.float())\n","  test_losses.append(test_loss.item())\n","\n","  highest_value = 0\n","  high = 0\n","  counter = 0\n","  for i in output[0].cpu().detach().numpy():\n","    if i > highest_value:\n","      highest_value = i\n","      high = counter\n","    counter += 1\n","  \n","  pred = [0,0,0,0]\n","  pred[high] = 1\n","\n","\n","  # convert output probabilities to predicted class (0 or 1)\n","  #pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","  # compare predictions to true label\n","  #correct_tensor = pred.eq(labels.view_as(pred))\n","\n","  if labels[0].numpy()[0]==1:\n","    total_s +=1\n","    if pred[0] == 1:\n","      num_sad_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[1]==1:\n","    total_a +=1\n","    if pred[1] == 1:\n","      num_anger_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[2]==1:\n","    total_f +=1\n","    if pred[2] == 1:\n","      num_fear_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[3]==1:\n","    total_d +=1\n","    if pred[3] == 1:\n","      num_disgust_correct += 1\n","      num_correct += 1\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","\n","test_acc = num_correct/len(test_loader.dataset)\n","test_acc_s = num_sad_correct/total_s\n","test_acc_f = num_fear_correct/total_f\n","test_acc_a = num_anger_correct/total_a\n","test_acc_d = num_disgust_correct/total_d\n","print(\"Test accuracy: {:.3f}\".format(test_acc))\n","print(\"Test sad accuracy: {:.6f}\".format(test_acc_s))\n","print(\"Test anger accuracy: {:.6f}\".format(test_acc_a))\n","print(\"Test fear accuracy: {:.6f}\".format(test_acc_f))\n","print(\"Test disgust accuracy: {:.6f}\".format(test_acc_d))"],"metadata":{"id":"MNdZV3BEM6VU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Ukraine Training"],"metadata":{"id":"FYpLjfA_X1md"}},{"cell_type":"markdown","source":["###Preparing Data Emoji Ukraine"],"metadata":{"id":"xLn6WY0HRYpm"}},{"cell_type":"code","source":["import re\n","from string import punctuation\n","\n","all_tweets_emoji_0 = list()\n","\n","for t in data_emoji_0[\"tweet\"]:\n","  #t = 'a ' + t\n","  t = re.sub(\"(?:\\s)@[^, ]*\", '', t)\n","  #t = re.sub(\"(?:\\s)#[^, ]*\", '', t)\n","  t = t[4:]\n","  t = t.lower()\n","  t = \"\".join([ch for ch in t if ch not in punctuation])\n","  all_tweets_emoji_0.append(t)\n","\n","all_text = \" \".join(all_tweets_emoji_0)\n","all_words = all_text.split()"],"metadata":{"id":"J7lkiOWsXrVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter \n","# Count all the words using Counter Method\n","count_words = Counter(all_words)\n","total_words=len(all_words)\n","sorted_words=count_words.most_common(total_words)\n","print(\"Top ten occuring words : \",sorted_words[:10])"],"metadata":{"id":"Lg6bhiiNX40q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}"],"metadata":{"id":"eReimhBZYCet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_tweets=list()\n","iter_1 = 0\n","iter_2 = 0\n","for t in all_tweets_emoji_0:\n","  encoded_tweet=list()\n","  for word in t.split():\n","    if word not in vocab_to_int.keys():\n","      #if word is not available in vocab_to_int put 0 in that place\n","      encoded_tweet.append(0)\n","      iter_1 += 1\n","    else:\n","      iter_2 += 1\n","      encoded_tweet.append(vocab_to_int[word])\n","  encoded_tweets.append(encoded_tweet)\n","print(iter_1)\n","print(iter_2)\n","print(encoded_tweets[0])"],"metadata":{"id":"I40UtWmiYHOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequence_length = 0\n","for i, tweet in enumerate(encoded_tweets):\n","  if len(tweet) > sequence_length:\n","    sequence_length = len(tweet)\n","\n","features_emoji_0 = []\n","for i, tweet in enumerate(encoded_tweets):\n","  tweet_len=len(tweet)\n","  if (tweet_len<=sequence_length):\n","    zeros=list(np.zeros(sequence_length-tweet_len))\n","    new=zeros+tweet\n","  else:\n","    new=tweet[:sequence_length]\n","  features_emoji_0.append(np.array(new))"],"metadata":{"id":"VIKY0e27YJ9O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Train Model"],"metadata":{"id":"2n6OcWZFRgUF"}},{"cell_type":"code","source":["train_x=features_emoji_0[:int(0.7*len(features_emoji_0))]\n","train_y=data_emoji_0[\"emotion\"][:int(0.7*len(features_emoji_0))]\n","valid_x=features_emoji_0[int(0.7*len(features_emoji_0)):int(0.8*len(features_emoji_0))]\n","valid_y=data_emoji_0[\"emotion\"][int(0.7*len(features_emoji_0)):int(0.8*len(features_emoji_0))]\n","test_x=features_emoji_0[int(0.8*len(features_emoji_0)):]\n","test_y=data_emoji_0[\"emotion\"][int(0.8*len(features_emoji_0)):]\n","print(len(train_y), len(valid_y), len(test_y))"],"metadata":{"id":"-rglAXKFYODf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","#create Tensor Dataset\n","train_data=TensorDataset(torch.FloatTensor(train_x).to(torch.int64), F.one_hot(torch.FloatTensor(train_y.to_numpy(dtype=np.float64)).to(torch.int64)))\n","valid_data=TensorDataset(torch.FloatTensor(valid_x).to(torch.int64), F.one_hot(torch.FloatTensor(valid_y.to_numpy(dtype=np.float64)).to(torch.int64)))\n","test_data=TensorDataset(torch.FloatTensor(test_x).to(torch.int64), F.one_hot(torch.FloatTensor(test_y.to_numpy(dtype=np.float64)).to(torch.int64)))\n","\n","#dataloader\n","batch_size=64\n","train_loader=DataLoader(train_data, batch_size=1, shuffle=True)\n","valid_loader=DataLoader(valid_data, batch_size=1, shuffle=True)\n","test_loader=DataLoader(test_data, batch_size=1, shuffle=True)"],"metadata":{"id":"ihU7eVyNYdWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# obtain one batch of training data\n","dataiter = iter(train_loader)\n","sample_x, sample_y = dataiter.next()\n","print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n","print('Sample input: \\n', sample_x)\n","print('Sample label size: ', sample_y.size()) # batch_size\n","print('Sample label: \\n', sample_y)"],"metadata":{"id":"LKaFQq16YjmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate the model w/ hyperparams\n","vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n","output_size = 4\n","embedding_dim = 400\n","hidden_dim = 256\n","n_layers = 1\n","\n","net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","print(net)"],"metadata":{"id":"s2IUk4S-Yn8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loss and optimization functions\n","lr=0.0005\n","\n","#criterion = nn.CrossEntropyLoss(reduction='sum')\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0001)\n","\n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","# training params\n","\n","epochs = 20# 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","training_loss=[]\n","validation_loss=[]\n","\n","counter = 0\n","print_every = 200\n","clip=5 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","for e in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    # batch loop\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            inputs=inputs.cuda()\n","            labels=labels.cuda()\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        net.zero_grad()\n","        # get the output from the model\n","        output, h = net(inputs, h)\n","        # calculate the loss and perform backprop\n","        loss = criterion(output, labels.float())\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","        # loss stats\n","        if counter % print_every == 0:\n","            # Get validation loss\n","            val_h = net.init_hidden(batch_size)\n","            val_losses = []\n","            net.eval()\n","            error = 0\n","            for inputs, labels in valid_loader:\n","\n","                # Creating new variables for the hidden state, otherwise\n","                # we'd backprop through the entire training history\n","                val_h = tuple([each.data for each in val_h])\n","                if(train_on_gpu):\n","                  inputs, labels = inputs.cuda(), labels.cuda()\n","\n","                output, val_h = net(inputs, val_h)\n","                val_loss = criterion(output, labels.float())\n","                val_losses.append(val_loss.item())\n","                 \n","            training_loss.append(loss)\n","            validation_loss.append(np.mean(val_losses))\n","            net.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Loss: {:.6f}...\".format(loss.item()),\n","                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"],"metadata":{"id":"fBCgn8GlYuXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1 = []\n","for i in training_loss:\n","  t1.append(i.cpu().item())\n","\n","\n","plt.plot(t1,'yellow', label=\"training loss\")\n","plt.plot(validation_loss, 'purple', label=\"validation loss\")\n","plt.legend()\n","plt.show()\n","\n","t = []\n","v = []\n","for i in range(0,20):\n","  v.append(np.mean(validation_loss[i*7:(i+1)*7]))\n","  t.append(np.mean(t1[i*7:(i+1)*7]))\n","\n","plt.plot(t,'red',label='training loss')\n","plt.plot(v, 'blue', label='validation loss')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"VdIDGow3Y1tc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Test Model"],"metadata":{"id":"b6bJ4u0pRjFF"}},{"cell_type":"code","source":["from functools import total_ordering\n","test_losses = [] # track loss\n","num_correct = 0\n","num_sad_correct = 0\n","num_anger_correct = 0\n","num_fear_correct = 0\n","num_disgust_correct = 0\n","total_s = 0\n","total_a = 0\n","total_f = 0\n","total_d = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.eval()\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","  # Creating new variables for the hidden state, otherwise\n","  # we'd backprop through the entire training history\n","  h = tuple([each.data for each in h])\n","  e=0\n","\n","  if(train_on_gpu):\n","    inputs, labels_2 = inputs.cuda(), labels.cuda()\n","\n","  output, h = net(inputs, h)\n","\n","  # calculate loss\n","  test_loss = criterion(output, labels_2.float())\n","  test_losses.append(test_loss.item())\n","\n","  highest_value = 0\n","  high = 0\n","  counter = 0\n","  for i in output[0].cpu().detach().numpy():\n","    if i > highest_value:\n","      highest_value = i\n","      high = counter\n","    counter += 1\n","  \n","  pred = [0,0,0,0]\n","  pred[high] = 1\n","\n","\n","  # convert output probabilities to predicted class (0 or 1)\n","  #pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","  # compare predictions to true label\n","  #correct_tensor = pred.eq(labels.view_as(pred))\n","\n","  if labels[0].numpy()[0]==1:\n","    total_s +=1\n","    if pred[0] == 1:\n","      num_sad_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[1]==1:\n","    total_a +=1\n","    if pred[1] == 1:\n","      num_anger_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[2]==1:\n","    total_f +=1\n","    if pred[2] == 1:\n","      num_fear_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[3]==1:\n","    total_d +=1\n","    if pred[3] == 1:\n","      num_disgust_correct += 1\n","      num_correct += 1\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","\n","test_acc = num_correct/len(test_loader.dataset)\n","test_acc_s = num_sad_correct/total_s\n","try:\n","  test_acc_f = num_fear_correct/total_f\n","except:\n","  test_acc_f = 0\n","test_acc_a = num_anger_correct/total_a\n","test_acc_d = num_disgust_correct/total_d\n","print(\"Test accuracy: {:.3f}\".format(test_acc))\n","print(\"Test sad accuracy: {:.6f}\".format(test_acc_s))\n","print(\"Test anger accuracy: {:.6f}\".format(test_acc_a))\n","print(\"Test disgust accuracy: {:.6f}\".format(test_acc_f))\n","print(\"Test fear accuracy: {:.6f}\".format(test_acc_d))"],"metadata":{"id":"TDKc4okwY6Gr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Ukraine Training Without Disgust"],"metadata":{"id":"emXJMz5FR2By"}},{"cell_type":"markdown","source":["###Preparing Data Emoji Ukraine"],"metadata":{"id":"dqtOjCD1R2Bz"}},{"cell_type":"code","source":["data_emoji_0[\"emotion\"].value_counts()\n","\n","print(data_emoji_0[\"emotion\"].value_counts())\n","data_emoji_0_s = data_emoji_0[data_emoji_0[\"emotion\"] == 0]\n","data_emoji_0_a = data_emoji_0[data_emoji_0[\"emotion\"] == 1]\n","data_emoji_0_f = data_emoji_0[data_emoji_0[\"emotion\"] == 2]\n","data_emoji_0_d = data_emoji_0[data_emoji_0[\"emotion\"] == 3]\n","\n","data_emoji_0_s = data_emoji_0_s.sample(n=46)\n","data_emoji_0_a = data_emoji_0_a.sample(n=46)\n","data_emoji_0_d = data_emoji_0_d.sample(n=46)\n","\n","data_emoji_0 = data_emoji_0_s.append(data_emoji_0_a,ignore_index=True)\n","data_emoji_0 = data_emoji_0.append(data_emoji_0_d,ignore_index=True)\n","\n","data_emoji_0 = shuffle(data_emoji_0)\n","\n","print(data_emoji_0[\"emotion\"].value_counts())"],"metadata":{"id":"766TDT1iR8OX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from string import punctuation\n","\n","all_tweets_emoji_0 = list()\n","\n","for t in data_emoji_0[\"tweet\"]:\n","  #t = 'a ' + t\n","  t = re.sub(\"(?:\\s)@[^, ]*\", '', t)\n","  #t = re.sub(\"(?:\\s)#[^, ]*\", '', t)\n","  t = t[4:]\n","  t = t.lower()\n","  t = \"\".join([ch for ch in t if ch not in punctuation])\n","  all_tweets_emoji_0.append(t)\n","\n","all_text = \" \".join(all_tweets_emoji_0)\n","all_words = all_text.split()"],"metadata":{"id":"8idQwwrzR2Bz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter \n","# Count all the words using Counter Method\n","count_words = Counter(all_words)\n","total_words=len(all_words)\n","sorted_words=count_words.most_common(total_words)\n","print(\"Top ten occuring words : \",sorted_words[:10])"],"metadata":{"id":"DR9IkmW_R2B0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}"],"metadata":{"id":"FwyQhdSLR2B1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_tweets=list()\n","iter_1 = 0\n","iter_2 = 0\n","for t in all_tweets_emoji_0:\n","  encoded_tweet=list()\n","  for word in t.split():\n","    if word not in vocab_to_int.keys():\n","      #if word is not available in vocab_to_int put 0 in that place\n","      encoded_tweet.append(0)\n","      iter_1 += 1\n","    else:\n","      iter_2 += 1\n","      encoded_tweet.append(vocab_to_int[word])\n","  encoded_tweets.append(encoded_tweet)\n","print(iter_1)\n","print(iter_2)\n","print(encoded_tweets[0])"],"metadata":{"id":"DPxGE3JwR2B1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequence_length = 0\n","for i, tweet in enumerate(encoded_tweets):\n","  if len(tweet) > sequence_length:\n","    sequence_length = len(tweet)\n","\n","features_emoji_0 = []\n","for i, tweet in enumerate(encoded_tweets):\n","  tweet_len=len(tweet)\n","  if (tweet_len<=sequence_length):\n","    zeros=list(np.zeros(sequence_length-tweet_len))\n","    new=zeros+tweet\n","  else:\n","    new=tweet[:sequence_length]\n","  features_emoji_0.append(np.array(new))"],"metadata":{"id":"aCcQ3pdpR2B2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Train Model"],"metadata":{"id":"m3MeiQ7aR2B2"}},{"cell_type":"code","source":["train_x=features_emoji_0[:int(0.7*len(features_emoji_0))]\n","train_y=data_emoji_0[\"emotion\"][:int(0.7*len(features_emoji_0))]\n","valid_x=features_emoji_0[int(0.7*len(features_emoji_0)):int(0.8*len(features_emoji_0))]\n","valid_y=data_emoji_0[\"emotion\"][int(0.7*len(features_emoji_0)):int(0.8*len(features_emoji_0))]\n","test_x=features_emoji_0[int(0.8*len(features_emoji_0)):]\n","test_y=data_emoji_0[\"emotion\"][int(0.8*len(features_emoji_0)):]\n","print(len(train_y), len(valid_y), len(test_y))"],"metadata":{"id":"nE47jUH3R2B3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","#create Tensor Dataset\n","train_data=TensorDataset(torch.FloatTensor(train_x).to(torch.int64), F.one_hot(torch.FloatTensor(train_y.to_numpy(dtype=np.float64)).to(torch.int64)))\n","valid_data=TensorDataset(torch.FloatTensor(valid_x).to(torch.int64), F.one_hot(torch.FloatTensor(valid_y.to_numpy(dtype=np.float64)).to(torch.int64)))\n","test_data=TensorDataset(torch.FloatTensor(test_x).to(torch.int64), F.one_hot(torch.FloatTensor(test_y.to_numpy(dtype=np.float64)).to(torch.int64)))\n","\n","#dataloader\n","batch_size=64\n","train_loader=DataLoader(train_data, batch_size=1, shuffle=True)\n","valid_loader=DataLoader(valid_data, batch_size=1, shuffle=True)\n","test_loader=DataLoader(test_data, batch_size=1, shuffle=True)"],"metadata":{"id":"Adk2YPivR2B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# obtain one batch of training data\n","dataiter = iter(train_loader)\n","sample_x, sample_y = dataiter.next()\n","print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n","print('Sample input: \\n', sample_x)\n","print('Sample label size: ', sample_y.size()) # batch_size\n","print('Sample label: \\n', sample_y)"],"metadata":{"id":"oG4bplV8R2B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate the model w/ hyperparams\n","vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n","output_size = 4\n","embedding_dim = 400\n","hidden_dim = 256\n","n_layers = 1\n","\n","net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","print(net)"],"metadata":{"id":"qSCqX2ckR2B5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loss and optimization functions\n","lr=0.0005\n","\n","#criterion = nn.CrossEntropyLoss(reduction='sum')\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0001)\n","\n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","# training params\n","\n","epochs = 20# 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","training_loss=[]\n","validation_loss=[]\n","\n","counter = 0\n","print_every = 200\n","clip=5 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","for e in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    # batch loop\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            inputs=inputs.cuda()\n","            labels=labels.cuda()\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        net.zero_grad()\n","        # get the output from the model\n","        output, h = net(inputs, h)\n","        # calculate the loss and perform backprop\n","        loss = criterion(output, labels.float())\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","        # loss stats\n","        if counter % print_every == 0:\n","            # Get validation loss\n","            val_h = net.init_hidden(batch_size)\n","            val_losses = []\n","            net.eval()\n","            error = 0\n","            for inputs, labels in valid_loader:\n","\n","                # Creating new variables for the hidden state, otherwise\n","                # we'd backprop through the entire training history\n","                val_h = tuple([each.data for each in val_h])\n","                if(train_on_gpu):\n","                  inputs, labels = inputs.cuda(), labels.cuda()\n","\n","                output, val_h = net(inputs, val_h)\n","                val_loss = criterion(output, labels.float())\n","                val_losses.append(val_loss.item())\n","                 \n","            training_loss.append(loss)\n","            validation_loss.append(np.mean(val_losses))\n","            net.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Loss: {:.6f}...\".format(loss.item()),\n","                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"],"metadata":{"id":"U126aoDIR2B5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t1 = []\n","for i in training_loss:\n","  t1.append(i.cpu().item())\n","\n","\n","plt.plot(t1,'yellow', label=\"training loss\")\n","plt.plot(validation_loss, 'purple', label=\"validation loss\")\n","plt.legend()\n","plt.show()\n","\n","t = []\n","v = []\n","for i in range(0,20):\n","  v.append(np.mean(validation_loss[i*7:(i+1)*7]))\n","  t.append(np.mean(t1[i*7:(i+1)*7]))\n","\n","plt.plot(t,'red',label='training loss')\n","plt.plot(v, 'blue', label='validation loss')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ziTAgxtcR2B6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Test Model"],"metadata":{"id":"NXLJXovoR2B7"}},{"cell_type":"code","source":["from functools import total_ordering\n","test_losses = [] # track loss\n","num_correct = 0\n","num_sad_correct = 0\n","num_anger_correct = 0\n","num_fear_correct = 0\n","num_disgust_correct = 0\n","total_s = 0\n","total_a = 0\n","total_f = 0\n","total_d = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.eval()\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","  # Creating new variables for the hidden state, otherwise\n","  # we'd backprop through the entire training history\n","  h = tuple([each.data for each in h])\n","  e=0\n","\n","  if(train_on_gpu):\n","    inputs, labels_2 = inputs.cuda(), labels.cuda()\n","\n","  output, h = net(inputs, h)\n","\n","  # calculate loss\n","  test_loss = criterion(output, labels_2.float())\n","  test_losses.append(test_loss.item())\n","\n","  highest_value = 0\n","  high = 0\n","  counter = 0\n","  for i in output[0].cpu().detach().numpy():\n","    if i > highest_value:\n","      highest_value = i\n","      high = counter\n","    counter += 1\n","  \n","  pred = [0,0,0,0]\n","  pred[high] = 1\n","\n","\n","  # convert output probabilities to predicted class (0 or 1)\n","  #pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","  # compare predictions to true label\n","  #correct_tensor = pred.eq(labels.view_as(pred))\n","\n","  if labels[0].numpy()[0]==1:\n","    total_s +=1\n","    if pred[0] == 1:\n","      num_sad_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[1]==1:\n","    total_a +=1\n","    if pred[1] == 1:\n","      num_anger_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[2]==1:\n","    total_f +=1\n","    if pred[2] == 1:\n","      num_fear_correct += 1\n","      num_correct += 1\n","  if labels[0].numpy()[3]==1:\n","    total_d +=1\n","    if pred[3] == 1:\n","      num_disgust_correct += 1\n","      num_correct += 1\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","\n","test_acc = num_correct/len(test_loader.dataset)\n","test_acc_s = num_sad_correct/total_s\n","test_acc_a = num_anger_correct/total_a\n","test_acc_d = num_disgust_correct/total_d\n","print(\"Test accuracy: {:.3f}\".format(test_acc))\n","print(\"Test sad accuracy: {:.6f}\".format(test_acc_s))\n","print(\"Test anger accuracy: {:.6f}\".format(test_acc_a))\n","print(\"Test fear accuracy: {:.6f}\".format(test_acc_d))"],"metadata":{"id":"bV5JG1iLR2B8"},"execution_count":null,"outputs":[]}]}